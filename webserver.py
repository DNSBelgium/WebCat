import uuid
from os import listdir
from pathlib import Path
from typing import Literal, Annotated

import pandas as pd
import pyarrow.parquet as pq
import structlog
import torch
from fastapi import (FastAPI, BackgroundTasks, status, File, UploadFile, Response, Form
                     )
from prometheus_client import Gauge, make_asgi_app
from pydantic import BaseModel, Field

from config import DEVICE
from model import load_model, make_predictions
from preprocess import preprocess_x, PreprocessedInputs

structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.processors.EventRenamer("msg"),
        structlog.processors.JSONRenderer(),
    ]
)

logger = structlog.get_logger()


class Condition:
    def __init__(self, err: Exception | None, work_in_progress: bool):
        self.err = err
        self.work_in_progress = work_in_progress


PREPARE_DICT = dict[str, Condition]()
PREDICTION_DICT = dict[str, Condition]()

MODEL_PATH = "./models"
PARQUET_INPUT_PATH = "./parquet_files_in"
HDF5_PATH = "./hdf5_files"
PARQUET_OUTPUT_PATH = "./parquet_files_out"

Path(MODEL_PATH).mkdir(parents=True, exist_ok=True)
Path(PARQUET_INPUT_PATH).mkdir(parents=True, exist_ok=True)
Path(HDF5_PATH).mkdir(parents=True, exist_ok=True)
Path(PARQUET_OUTPUT_PATH).mkdir(parents=True, exist_ok=True)


class PrepareJson(BaseModel):
    model: str = Field(description="The model file name for the model that will be used to predict/prepare.")
    visit_id: list[str] = Field(default=[], description="Row ids for to be processed items. "
                                                        "The list MUST be in the same order as the given domain names."
                                                        "If not given, a list of {domain name}-{incremental number} is "
                                                        "set automatically.")
    domain_name: list[str] = Field(description="A list of domain names")
    body_text: list[str] = Field(
        description="A list of body texts (title concat parsed body html). The list MUST be in the same order as the "
                    "given domain names.")
    meta_text: list[str] = Field(
        description="A list of meta  (keywords concat description)."
                    " The list MUST be in the same order as the given domain names.")
    external_hosts: list[str] = Field(default=[], description="A list of external hosts."
                                                              " The list MUST be in the same order as the given domain names."
                                                              " If not given, a list of empy strings will be generated.")
    job_id: str = Field(default='')
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "model": "model_file.model",
                    "visit_id": ["example-1.de", "example-2.de"],
                    "domain_name": ["example-1.de", "example-2.de"],
                    "body_text": ["the body from the first example-1.de", "the body from the first example-2.de"],
                    "meta_text": ["meta-example-1.de", "meta-example-2.de"],
                    "external_hosts": [],
                }
            ]
        }
    }


class PrepareParquet(BaseModel):
    model: str = Field(description="The model file name for the model that will be used to predict/prepare."),
    job_id: str = Field(default='')
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "model": "model_file.model",

                }
            ]
        }
    }


class Predict(BaseModel):
    model: str = Field(description="The model file name for the model that will be used to predict/prepare.")
    job_id: str = Field(description="The unique id which was generated by the prepare process")
    model_config = {
        "json_schema_extra": {
            "examples": [{
                "model": "model_file.model",
                "job_id": str(uuid.uuid4()),
            }]
        }
    }


class PrepareStatus(BaseModel):
    job_id: str = Field(description="The unique id which was generated by the prepare process")
    message: str = Field(description="The description of the actual status")


class PredictionData(BaseModel):
    visit_id: list[str] = Field(description="")
    category: list[str] = Field(description="")


class PredictStatus(BaseModel):
    job_id: str = Field(description="unique id which was generated by the prepare process")
    message: str = Field(description="description of the actual status")
    data: PredictionData | None = Field(description="Data of predict process if finished")


class PredictPostResponse(BaseModel):
    job_id: str = Field(description="unique id for job")
    message: str = Field(description="description")


class PreparePostResponse(BaseModel):
    job_id: str = Field(description="unique id for job")
    message: str = Field(description="description")


description = """
    This REST API takes REST requests and provide it to the WebCat Service which just accepts
    parquet files. The API service convert the http request in a WebCat understandable format.
    
    The process is split into two parts:
    
    1. Prepare: The given data get analyzed and packed into a parquet file which can be provided to the
                WebCat service
    2. Predict: The generated parquet file can be send to the WebCat service which will start the prediction process.
    
    Those two steps are independent, so it is possible to start several prepare processes independently from the predict
    process.
    
    All prepared parquet files are stored on the file system an can be handled via the RESTvAPI  
      
"""

app = FastAPI(
    title="REST API endpoint for the WebCat service",
    description=description,
    version="0.0.1",
    servers=[
        {"url": "http://localhost:8000", "description": "Test WebCat service"},
    ]
)


# toDo Loging


@app.get("/", )
def read_root():
    return Response(status_code=status.HTTP_204_NO_CONTENT)


@app.get("/prepare/{job_id}",
         summary="Get the status of a specific currently running prepare process",
         description="Get the current status of a given job_id in prepare step.",
         responses={
             200: {
                 "model": PrepareStatus,
                 "content": {
                     "application/json": {
                         "example": {
                             "job_id": str(uuid.uuid4()),
                             "message": "job done",
                         }
                     }
                 },
             },
             400: {
                 "model": PrepareStatus,
                 "description": "The given job id was not found.",
                 "content": {
                     "application/json": {
                         "example": {
                             "job_id": str(uuid.uuid4()),
                             "message": "job not found",
                         }
                     }
                 },
             },
             423: {
                 "model": PrepareStatus,
                 "description": "There is actually a running job",
                 "content": {
                     "application/json": {
                         "example": {
                             "job_id": str(uuid.uuid4()),
                             "message": "job in progress",
                         }
                     }
                 },
             },
             500: {
                 "model": PrepareStatus,
                 "content": {
                     "application/json": {
                         "example": {
                             "job_id": str(uuid.uuid4()),
                             "message": "something went wrong: {reason}",
                         }
                     }
                 }
             }
         },

         )
async def get_status_prepare(job_id: str, response: Response) -> PrepareStatus:
    look_up = PREPARE_DICT

    if job_id in look_up and look_up[job_id].work_in_progress:
        response.status_code = status.HTTP_423_LOCKED
        return PrepareStatus(
            job_id=job_id,
            message="job in progress",
            status_code=status.HTTP_423_LOCKED,
        )

    elif job_id in look_up and look_up[job_id].err is not None:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return PrepareStatus(
            job_id=job_id, message=f"something went wrong: {look_up[job_id].err}",
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
        )

    elif job_id in look_up:
        response.status_code = status.HTTP_200_OK
        return PrepareStatus(job_id=job_id, message="job done")
    else:
        response.status_code = status.HTTP_400_BAD_REQUEST
        return PrepareStatus(job_id=job_id, message="job not found")


@app.post(
    "/prepare/parquet",
    summary="Trigger a job preparation",
    description="Trigger prepare step by sending a parquet file",
    responses={
        201: {
            "model": PrepareStatus,
            "content": {
                "application/json": {
                    "example": {
                        "job_id": str(uuid.uuid4()),
                        "message": "prepare started",
                    }
                },
            },
        },
        400: {
            "model": PrepareStatus,
            "content": {
                "application/json": {
                    "example": {
                        "job_id": "",
                        "message": "model not found",
                    }
                },
            },
        },
        423: {
            "model": PrepareStatus,
            "content": {
                "application/json": {
                    "example": {
                        "job_id": "",
                        "message": "preparation in progress",
                    }
                },
            },
        },
        500: {
            "model": PrepareStatus,
            "content": {
                "application/json": {
                    "example": {
                        "job_id": "",
                        "message": "there was an error uploading the parquet file",
                    }
                },
            },
        },
    }

)
async def prepare_parquet(model: Annotated[str, Form()], file: Annotated[UploadFile, File()],
                          background_tasks: BackgroundTasks, res: Response,
                          ):
    if not Path(f"{MODEL_PATH}/{model}").exists():
        result = PredictPostResponse(message="model not found", job_id="")
        res.status_code = status.HTTP_400_BAD_REQUEST
        return result
    if is_work_in_progress(PREPARE_DICT):
        result = PredictPostResponse(message="preparation in progress", job_id="")
        res.status_code = status.HTTP_423_LOCKED
        return result
    else:
        job_id = f"{uuid.uuid4()}"
        try:
            contents = file.file.read()
            with open(f"{PARQUET_INPUT_PATH}/{job_id}.parquet", 'wb') as f:
                f.write(contents)
        except Exception:
            result = PredictPostResponse(message="there was an error uploading the parquet file",
                                         job_id="")
            res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
            return result
        finally:
            if isinstance(file, UploadFile):
                file.file.close()
        result = PredictPostResponse(message="prepare started",
                                     job_id=job_id)
        res.status_code = status.HTTP_201_CREATED
        background_tasks.add_task(do_preparation, job_id, model)

        return result


@app.post(
    "/prepare/json",
    summary="Trigger a job preparation",
    description="Trigger prepare step by sending a parquet file",
    responses={
        201: {
            "model": PrepareStatus,
            "content": {
                "application/json": {
                    "example": {
                        "job_id": str(uuid.uuid4()),
                        "message": "prepare started",
                    }
                },
            },
        },
        400: {
            "model": PrepareStatus,
            "content": {
                "application/json": {
                    "example": {
                        "job_id": "",
                        "message": "model not found",
                    }
                },
            },
        },
        423: {
            "model": PrepareStatus,
            "content": {
                "application/json": {
                    "example": {
                        "job_id": "",
                        "message": "preparation in progress",
                    }
                },
            },
        },
        500: {
            "model": PrepareStatus,
            "content": {
                "application/json": {
                    "example": {
                        "job_id": "",
                        "message": "there was an error uploading the parquet file",
                    }
                },
            },
        },
    }
)
async def prepare_json(args: PrepareJson, background_tasks: BackgroundTasks, res: Response) -> PreparePostResponse:
    if not Path(f"{MODEL_PATH}/{args.model}").exists():
        result = PreparePostResponse(message="model not found", job_id="")
        res.status_code = status.HTTP_400_BAD_REQUEST
        return result
    if is_work_in_progress(PREPARE_DICT):
        result = PreparePostResponse(message="job in progress",
                                     job_id="")
        res.status_code = status.HTTP_423_LOCKED
        return result
    else:
        args.job_id = f"{uuid.uuid4()}"
        try:
            write_preparation_parquet_from_body(args)
        except ValueError as ex:
            result = PreparePostResponse(message=f"input data was not correct: {ex}",
                                         job_id="")
            res.status_code = status.HTTP_406_NOT_ACCEPTABLE
            return result
        background_tasks.add_task(do_preparation, args.job_id,args.model)
        result = PreparePostResponse(message="prepare started",
                                     job_id=args.job_id)
        res.status_code = status.HTTP_201_CREATED
        return result


@app.get("/predict/{job_id}",
         response_model=PredictStatus,
         status_code=status.HTTP_200_OK,
         summary="Get the status of a specific currently running predict process",
         description="Get the current status of a running predict process",
         responses={
             200: {
                 "model": PredictStatus,
                 "content": {
                     "application/json": {
                         "example": {
                             "job_id": str(uuid.uuid4()),
                             "message": "job done",
                             "data": {
                                 "visit_id": [
                                     "example-1.de",
                                     "example-2.de"
                                 ],
                                 "category": [
                                     "IT",
                                     "IT"
                                 ]
                             }
                         }
                     }
                 },
             },
             400: {
                 "model": PredictStatus,
                 "description": "The given job id was not found.",
                 "content": {
                     "application/json": {
                         "example": {
                             "job_id": "",
                             "message": "job not found",
                             "data": ""
                         }
                     }
                 },
             },
             423: {
                 "model": PredictStatus,
                 "description": "There is actually a running job",
                 "content": {
                     "application/json": {
                         "example": {
                             "job_id": "",
                             "message": "job in progress",
                             "data": ""
                         }
                     }
                 },
             },
             500: {
                 "model": PredictStatus,
                 "content": {
                     "application/json": {
                         "example": {
                             "job_id": "",
                             "message": "something went wrong: {reason}",
                             "data": ""
                         }
                     }
                 }
             }
         },
         )
async def get_predict(job_id: str, res: Response) -> PredictStatus:
    if job_id in PREDICTION_DICT and PREDICTION_DICT[job_id].work_in_progress:
        result = PredictStatus(
            job_id=job_id, message="job in progress", data=None
        )
        res.status_code = status.HTTP_423_LOCKED
        return result

    elif job_id in PREDICTION_DICT and PREDICTION_DICT[job_id].err is not None:
        result = PredictStatus(
            job_id=job_id, message=f"something went wrong: {PREDICTION_DICT[job_id].err}",
            data=None
        )
        res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return result
    elif job_id in PREDICTION_DICT:
        parquet_res = parquet_to_json(job_id)
        if parquet_res is not None:
            result = PredictStatus(job_id=job_id, message="job done", data=parquet_res)
            res.status_code = status.HTTP_200_OK
            return result
        else:
            result = PredictStatus(job_id=job_id, message="job is finished, but could not find result data",
                                   data=None)
            res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
            return result
    else:
        result = PredictStatus(job_id=job_id, message="job not found",
                               data=None)
        res.status_code = status.HTTP_400_BAD_REQUEST
        return result


@app.post("/predict",
          response_model=PredictPostResponse,
          status_code=status.HTTP_201_CREATED,
          summary="Start predict process",
          description="This Endpoint start a predict process.",
          responses={
              201: {
                  "model": PrepareStatus,
                  "content": {
                      "application/json": {
                          "example": {
                              "job_id": str(uuid.uuid4()),
                              "message": "job started",
                          }
                      },
                  },
              },
              400: {
                  "model": PrepareStatus,
                  "content": {
                      "application/json": {
                          "example": {
                              "job_id": str(uuid.uuid4()),
                              "message": "model not found",
                          }
                      },
                  },
              },
              423: {
                  "model": PrepareStatus,
                  "content": {
                      "application/json": {
                          "example": {
                              "job_id": str(uuid.uuid4()),
                              "message": "job in progress",
                          }
                      },
                  },
              },
          }
          )
async def predict(args: Predict, background_tasks: BackgroundTasks, res: Response) -> PredictPostResponse:
    if not Path(f"{MODEL_PATH}/{args.model}").exists():
        result = PredictPostResponse(message="model not found", job_id=args.job_id)
        res.status_code = status.HTTP_400_BAD_REQUEST
        return result
    if is_work_in_progress(PREDICTION_DICT):
        result = PredictPostResponse(message="job in progress",
                                     job_id=args.job_id)
        res.status_code = status.HTTP_423_LOCKED
        return result
    else:
        background_tasks.add_task(do_prediction, args)
        result = PredictPostResponse(message="job started",
                                     job_id=args.job_id)
        res.status_code = status.HTTP_201_CREATED
        return result


@app.get(
    "/model",
    status_code=status.HTTP_200_OK,
    summary="Return a list of available models",
    responses={
        200: {
            "content": {
                "application/json": {
                    "example": {
                        "files": ["model.parquet", "model2.parquet"],

                    }
                }
            }
        }
    }
)
async def get_models():
    files = listdir(MODEL_PATH)
    return {"files": files}


@app.post("/model",
          status_code=status.HTTP_201_CREATED,
          summary="Upload a model",
          description="Upload a model. The model will be overwritten if a file with the same name already exists",
          responses={
              201: {

                  "content": {
                      "application/json": {
                          "example": {
                              "message": "successfully uploaded model {filename}",
                          }
                      }
                  }
              },
              500: {
                  "content": {
                      "application/json": {
                          "example": {
                              "message": "there was an error uploading the model",
                          }
                      }
                  }
              }
          }
          )
async def upload_model(res: Response, file: UploadFile = File(...)):
    try:
        contents = await file.file.read()
        with open(f"{MODEL_PATH}/{file.filename}", 'wb') as f:
            f.write(contents)
    except Exception:
        res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"message": "there was an error uploading the model"}
    finally:
        await file.file.close()
    res.status_code = status.HTTP_201_CREATED
    return {"message": f"successfully uploaded model {file.filename}"}


@app.delete(
    "/model/{file}",
    summary="Delete a model",
    responses={
        200: {

            "content": {
                "application/json": {
                    "example": {
                        "message": "deleted successfully",
                    }
                }
            }
        },
        500: {
            "content": {
                "application/json": {
                    "example": {
                        "message": "there was an error deleting this model",
                    }
                }
            }
        }
    }
)
async def delete_models(file: str | None, res: Response):
    try:
        Path(MODEL_PATH, file).unlink()
        res.status_code = status.HTTP_200_OK
        return {"message": "deleted successfully"}
    except Exception:
        res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"message": "there was an error deleting this model"}


@app.get("/parquet-input-file",
         status_code=status.HTTP_200_OK,
         summary="Return a list of available parquet input files",
         responses={
             200: {
                 "content": {
                     "application/json": {
                         "example": {
                             "files": ["file1.parquet", "file1.parquet"],

                         }
                     }
                 }
             }
         }
         )
async def get_input_files():
    files = listdir(PARQUET_INPUT_PATH)
    return {"files": files}


@app.delete("/parquet-input-file/{file}",
            summary="Delete a specific parquet input file",
            responses={
                200: {
                    "content": {
                        "application/json": {
                            "example": {
                                "message": "deleted successfully"
                            }
                        }
                    }
                },
                500: {
                    "content": {
                        "application/json": {
                            "example": {
                                "message": "there was an error deleting this parquet file"
                            }
                        }
                    }
                }
            }
            )
async def delete_input_files(file: str | None, res: Response):
    try:
        Path(PARQUET_INPUT_PATH, file).unlink()
        res.status_code = status.HTTP_200_OK
        return {"message": "deleted successfully"}
    except Exception:
        res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"message": "there was an error deleting this parquet file"}


@app.get("/parquet-output-file",
         status_code=status.HTTP_200_OK,
         summary="Return a list of available parquet output files",
         responses={
             200: {
                 "content": {
                     "application/json": {
                         "example": {
                             "files": ["file1.parquet", "file1.parquet"],

                         }
                     }
                 }
             }
         }
         )
async def get_output_files():
    files = listdir(PARQUET_OUTPUT_PATH)
    return {"files": files}


@app.delete("/parquet-output-file/{file}",
            summary="Delete a specific parquet output file",
            responses={
                200: {
                    "content": {
                        "application/json": {
                            "example": {
                                "message": "deleted successfully"
                            }
                        }
                    }
                },
                500: {
                    "content": {
                        "application/json": {
                            "example": {
                                "message": "there was an error deleting this parquet file"
                            }
                        }
                    }
                }
            }
            )
async def delete_output_files(file: str | None, res: Response):
    try:
        Path(PARQUET_OUTPUT_PATH, file).unlink()
        res.status_code = status.HTTP_200_OK
        return {"message": "deleted successfully"}
    except Exception:
        res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"message": "there was an error deleting this parquet file"}


@app.get("/hdf5-file",
         status_code=status.HTTP_200_OK,
         summary="Return a list of available hd5 files",
         responses={
             200: {
                 "content": {
                     "application/json": {
                         "example": {
                             "files": ["file1.hdf5", "file1.hdf5"],

                         }
                     }
                 }
             }
         }
         )
async def get_hdf5_files():
    return {"files": listdir(HDF5_PATH)}


@app.delete("/hdf5-file/{file}",
            summary="Delete a specific hdf5 file",
            responses={
                200: {
                    "content": {
                        "application/json": {
                            "example": {
                                "message": "deleted successfully"
                            }
                        }
                    }
                },
                500: {
                    "content": {
                        "application/json": {
                            "example": {
                                "message": "there was an error deleting this parquet file"
                            }
                        }
                    }
                }
            }
            )
async def delete_hdf5_files(file: str | None, res: Response = None):
    try:
        Path(HDF5_PATH, file).unlink()
        res.status_code = status.HTTP_200_OK
        return {"message": "deleted successfully"}
    except Exception:
        res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"message": "there was an error deleting this hd5 file"}


@app.get("/job",
         summary="Get running jobs",
         responses={
             200: {
                 "content": {
                     "application/json": {
                         "example": {
                             "prepare": [
                                 {"job3": {"err": "some error", "work_in_progress": False}},
                                 {"job4": {"err": None, "work_in_progress": True}},

                             ],
                             "predict": [
                                 {"job1": {"err": None, "work_in_progress": False}},
                                 {"job2": {"err": None, "work_in_progress": True}},
                             ]
                         }
                     }
                 }
             }
         }
         )
async def get_jobs():
    global PREPARE_DICT
    global PREDICTION_DICT

    return {
        "jobs":
            {
                "prepare": PREPARE_DICT,
                "predict": PREDICTION_DICT,
            },
    }


@app.delete("/job/{job_id}",
            summary="Delete a specific job",
            responses={
                200: {
                    "content": {
                        "application/json": {
                            "example": {
                                "message": "deleted successfully"
                            }
                        }
                    }
                },
                423: {
                    "content": {
                        "application/json": {
                            "example": {
                                "message": "job in progress for: {job_id}"
                            }
                        }
                    }
                },
                500: {
                    "content": {
                        "application/json": {
                            "example": {
                                "message": "could not delete file: {job_id}"
                            }
                        }
                    }
                }
            }
            )
async def delete_job(job_id: str, res: Response):
    global PREPARE_DICT
    global PREDICTION_DICT

    if is_job_in_progress(PREPARE_DICT, job_id) or is_job_in_progress(PREDICTION_DICT, job_id):
        res.status_code = status.HTTP_423_LOCKED
        return {"deleted": [], "message": f"job in progress for: {job_id}"}

    if job_id in PREPARE_DICT:
        PREPARE_DICT.pop(job_id)
    if job_id in PREDICTION_DICT:
        PREDICTION_DICT.pop(job_id)

    deleted = []
    for fp in listdir(PARQUET_INPUT_PATH):
        file_name = f"{job_id}.parquet"
        if file_name in fp:
            path = Path(PARQUET_INPUT_PATH, fp)
            try:
                path.unlink()
            except Exception:
                res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
                return {"deleted": deleted, "error": f"could not delete file: {job_id}"}
            deleted.append(f"input_file: {file_name}")

    for fp in listdir(PARQUET_OUTPUT_PATH):
        file_name = f"{job_id}.parquet"
        if file_name in fp:
            path = Path(PARQUET_OUTPUT_PATH, fp)
            try:
                path.unlink()
            except Exception:
                res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
                return {"deleted": deleted, "error": f"could not delete file: {job_id}"}
            deleted.append(f"output_file: {file_name}")

    for fp in listdir(HDF5_PATH):
        file_name = f"{job_id}.hdf5"
        if file_name in fp:
            path = Path(HDF5_PATH, fp)
            try:
                path.unlink()
            except Exception:
                res.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
                return {"deleted": deleted, "error": f"could not delete file: {job_id}"}
            deleted.append(f"hdf5_file: {file_name}")

    res.status_code = status.HTTP_200_OK
    return {"deleted": deleted, "message": f"removed all files for job: {job_id}"}


def do_preparation(job_id: str, model: str):
    global PREPARE_DICT
    PREPARE_DICT[job_id] = Condition(None, True)
    try:
        dit_data_preparation(model_path=f"{MODEL_PATH}/{model}",
                             parquet_path=f"{PARQUET_INPUT_PATH}/{job_id}.parquet",
                             hd5_path=f"{HDF5_PATH}/{job_id}.hdf5"
                             )
        PREPARE_DICT[job_id] = Condition(None, False)
    except Exception as ex:
        PREPARE_DICT[job_id] = Condition(ex, False)


def dit_data_preparation(model_path, parquet_path, hd5_path):
    col_transf = torch.load(model_path, map_location=DEVICE)[0]
    with pq.ParquetFile(parquet_path) as pqf:
        preprocess_x(pqf, col_transf, hd5_path)


def do_prediction(args: Predict):
    global PREDICTION_DICT
    PREDICTION_DICT[args.job_id] = Condition(None, True)
    try:
        dit_category_prediction(model_path=f"{MODEL_PATH}/{args.model}",
                                hd5_path=f"{HDF5_PATH}/{args.job_id}.hdf5",
                                parquet_path=f"{PARQUET_OUTPUT_PATH}/{args.job_id}.parquet"
                                )
        PREDICTION_DICT[args.job_id] = Condition(None, False)
    except Exception as ex:
        PREDICTION_DICT[args.job_id] = Condition(ex, False)


def dit_category_prediction(model_path, hd5_path, parquet_path):
    _, model, binary_threshold, label_encoder = load_model(model_path)
    inputs = PreprocessedInputs.load(hd5_path)

    output_type: Literal["binary", "multi"] = "binary" if len(label_encoder.classes_) == 2 else "multi"
    print("Starting predictions.")
    make_predictions(inputs,
                     model,
                     label_encoder,
                     output_type,
                     False,
                     binary_threshold,
                     parquet_path,
                     True)


def is_work_in_progress(resource_dict):
    for _, value in resource_dict.items():
        if value.work_in_progress:
            return True
    return False


def is_job_in_progress(resource_dict, job_id):
    if job_id in resource_dict:
        condition = resource_dict[job_id]
        return condition.work_in_progress
    return False


def write_preparation_parquet_from_body(prep: PrepareJson):
    if len(prep.visit_id) == 0:
        prep.visit_id = [f"{prep.domain_name[i]}-{i}" for i in range(len(prep.domain_name))]

    if len(prep.external_hosts) == 0:
        prep.external_hosts = ["" for _ in prep.domain_name]

    if len(prep.domain_name) != len(prep.body_text):
        raise ValueError('domain_name and body_text diff length')
    if len(prep.domain_name) != len(prep.meta_text):
        raise ValueError('domain_name and meta_text diff length')
    if len(prep.domain_name) != len(prep.visit_id):
        raise ValueError('domain_name and visit_id diff length')
    if len(prep.domain_name) != len(prep.external_hosts):
        raise ValueError('domain_name and external_hosts diff length')

    df = pd.DataFrame({'visit_id': prep.visit_id,
                       'domain_name': prep.domain_name,
                       'body_text': prep.body_text,
                       'meta_text': prep.meta_text,
                       'external_hosts': prep.external_hosts})

    df.to_parquet(f'{PARQUET_INPUT_PATH}/{prep.job_id}.parquet')


def parquet_to_json(job_id: str) -> PredictionData:
    for fp in listdir(PARQUET_OUTPUT_PATH):
        if job_id in fp:
            df = pd.read_parquet(f"{PARQUET_OUTPUT_PATH}/{fp}")
            result = PredictionData(visit_id=df['visit_id'].to_list(), category=df['predicted_label'].to_list())
            return result


### Prometheus handling

metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

metric_available = Gauge("CUDA_is_available", "Is the CUDA device available?")
metric_initialized = Gauge("CUDA_is_initialized", "Is the CUDA device initialized?")
metric_memory_usage = Gauge("CUDA_device_memory_usage", "CUDA memory usage")
metric_utilization = Gauge("CUDA_device_utilization", "CUDA utilization")
metric_temperature = Gauge("CUDA_device_temperature", "CUDA temperature")

try:
    metric_available.set(float(torch.cuda.is_available()))
    metric_initialized.set(float(torch.cuda.is_initialized()))
    metric_memory_usage.set(float(torch.cuda.memory_usage()))
    metric_utilization.set(float(torch.cuda.utilization()))
    metric_temperature.set(float(torch.cuda.temperature()))
except Exception as ex:
    metric_available.set(0.0)
    metric_initialized.set(0.0)
    metric_memory_usage.set(0.0)
    metric_utilization.set(0.0)
    metric_temperature.set(0.0)
    print(f"cloud not import get data from cuda driver: {ex}")
